# Dockerfile.vllm
# GPU-enabled vLLM inference server for bench-race
#
# Build (default CUDA 12.1):
#   docker build -f deploy/Dockerfile.vllm -t bench-race-vllm .
#
# Build for GB10 / CUDA 12.8:
#   docker build -f deploy/Dockerfile.vllm --build-arg CU_VERSION=cu128 -t bench-race-vllm .
#
# Run:
#   docker run --gpus all -p 8000:8000 -v /mnt/models/vllm:/models bench-race-vllm \
#     --model meta-llama/Llama-3.1-8B-Instruct

FROM nvidia/cuda:12.1.1-devel-ubuntu22.04 AS base

ARG CU_VERSION=cu121

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 \
    python3-pip \
    python3-venv \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN groupadd -r vllm && useradd -r -g vllm -m vllm

# Use a venv to avoid polluting system Python
ENV VIRTUAL_ENV=/opt/vllm-venv
RUN python3 -m venv $VIRTUAL_ENV
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

# Install PyTorch + vLLM inside venv
RUN pip install --no-cache-dir \
    torch --index-url https://download.pytorch.org/whl/${CU_VERSION} \
    && pip install --no-cache-dir vllm uvicorn

# Create model directory
RUN mkdir -p /models && chown vllm:vllm /models

# Health check
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD curl -sf http://localhost:8000/health || exit 1

USER vllm
WORKDIR /home/vllm

EXPOSE 8000

# Default entrypoint - pass model via --model flag
ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server", \
            "--host", "0.0.0.0", "--port", "8000", "--download-dir", "/models"]
